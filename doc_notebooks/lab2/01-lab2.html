

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Stability and accuracy (Jan. 2020) &mdash; Numeric course 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Laboratory 3: Linear Algebra (Sept. 12, 2017)" href="../lab3/01-lab3.html" />
    <link rel="prev" title="Laboratory 1: An Introduction to the Numerical Solution of Differential Equations: Discretization (2020/Jan/6)" href="../lab1/01-lab1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> Numeric course
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../course_bootstrap.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ugradsyllabus20.html">Undergrad Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gradsyllabus20.html">Grad Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../schedule20.html">Detailed Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../texts.html">Optional texts</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../notebook_toc.html">Course/lab descriptions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../lab1/01-lab1.html">Lab 1</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Lab 2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#List-of-Problems">List of Problems</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Objectives">Objectives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Readings">Readings</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Other-recommended-books">Other recommended books</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Accuracy-of-Difference-Approximations">Accuracy of Difference Approximations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Round-off-Error-and-Discretization-Error">Round-off Error and Discretization Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Problem-Error-–-hand-in-png-file-or-paper">Problem Error – hand in png file or paper</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Other-Approximations-to-the-First-Derivative">Other Approximations to the First Derivative</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Accuracy-Summary">Accuracy Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Stability-of-Difference-Approximations">Stability of Difference Approximations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Stiff-Equations">Stiff Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Difference-Approximations-of-Higher-Derivatives">Difference Approximations of Higher Derivatives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Mathematical-Notes">Mathematical Notes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Taylor-Polynomials-and-Taylor-Series">Taylor Polynomials and Taylor Series</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Floating-Point-Representation-of-Numbers">Floating Point Representation of Numbers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../lab3/01-lab3.html">Lab 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab4/01-lab4.html">Table of Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab4/01-lab4.html#Solving-Ordinary-Differential-Equations-with-the-Runge-Kutta-Methods">Solving Ordinary Differential Equations with the Runge-Kutta Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab5/01-lab5.html">Lab 5</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab6/lab6.html">Contents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../lab6/lab6.html#List-of-Problems">List of Problems</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../quizzes.html">Quizzes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Numeric course</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../notebook_toc.html">Numeric notebooks</a> &raquo;</li>
        
      <li>Stability and accuracy (Jan. 2020)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/doc_notebooks/lab2/01-lab2.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<h1><p>Table of Contents</p>
</h1><div class="toc"><ul class="toc-item"><li><p>1  Stability and accuracy (Jan. 2020)</p>
<ul class="toc-item"><li><p>1.1  List of Problems</p>
</li><li><p>1.2  Objectives</p>
</li><li><p>1.3  Readings</p>
<ul class="toc-item"><li><p>1.3.1  Other recommended books</p>
</li></ul></li><li><p>1.4  Introduction</p>
</li><li><p>1.5  Accuracy of Difference Approximations</p>
<ul class="toc-item"><li><p>1.5.1  Round-off Error and Discretization Error</p>
</li><li><p>1.5.2  Problem Error – hand in png file or paper</p>
</li><li><p>1.5.3  Other Approximations to the First Derivative</p>
</li><li><p>1.5.4  Accuracy Summary</p>
</li></ul></li><li><p>1.6  Stability of Difference Approximations</p>
</li><li><p>1.7  Stiff Equations</p>
</li><li><p>1.8  Difference Approximations of Higher Derivatives</p>
</li><li><p>1.9  Summary</p>
</li><li><p>1.10  Mathematical Notes</p>
<ul class="toc-item"><li><p>1.10.1  Taylor Polynomials and Taylor Series</p>
</li><li><p>1.10.2  Floating Point Representation of Numbers</p>
</li></ul></li></ul></li></ul></div><div class="section" id="Stability-and-accuracy-(Jan. 2020)">
<h1>Stability and accuracy (Jan. 2020)<a class="headerlink" href="#Stability-and-accuracy-(Jan. 2020)" title="Permalink to this headline">¶</a></h1>
<p>Both grad and undergrad should do each of the 5 problems below and upload their solutions to canvas.</p>
<div class="section" id="List-of-Problems">
<h2>List of Problems<a class="headerlink" href="#List-of-Problems" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#Problem-Error">Problem Error</a></p></li>
<li><p><a class="reference external" href="#Problem-Accuracy">Problem Accuracy</a></p></li>
<li><p><a class="reference external" href="#Problem-Stability">Problem Stability</a></p></li>
<li><p><a class="reference external" href="#Problem-Backward-Euler">Problem Backward-Euler</a></p></li>
<li><p><a class="reference external" href="#Problem-Taylor">Problem Taylor</a></p></li>
</ul>
</div>
<div class="section" id="Objectives">
<h2>Objectives<a class="headerlink" href="#Objectives" title="Permalink to this headline">¶</a></h2>
<p>In Lab #1 you were introduced to the concept of discretization, and saw that there were many different ways to approximate a given problem. This Lab will delve further into the concepts of accuracy and stability of numerical schemes, in order that we can compare the many possible discretizations.</p>
<p>At the end of this Lab, you will have seen where the error in a numerical scheme comes from, and how to quantify the error in terms of <em>order</em>. The stability of several examples will be demonstrated, so that you can recognize when a scheme is unstable, and how one might go about modifying the scheme to eliminate the instability.</p>
<p>Specifically you will be able to:</p>
<ul class="simple">
<li><p>Define the term and identify: Implicit numerical scheme and Explicit numerical scheme.</p></li>
<li><p>Define the term, identify, or write down for a given equation: Backward Euler method and Forward Euler method.</p></li>
<li><p>Explain the difference in terminology between: Forward difference discretization and Forward Euler method.</p></li>
<li><p>Define: truncation error, local truncation error, global truncation error, and stiff equation.</p></li>
<li><p>Explain: a predictor-corrector method.</p></li>
<li><p>Identify from a plot: an unstable numerical solution.</p></li>
<li><p>Be able to: find the order of a scheme, using the test equation find the stability of a scheme, find the local truncation error from a graph of the exact solution and the numerical solution.</p></li>
</ul>
</div>
<div class="section" id="Readings">
<h2>Readings<a class="headerlink" href="#Readings" title="Permalink to this headline">¶</a></h2>
<p>This lab is designed to be self-contained. If you would like additional background on any of the following topics, I’d recommend this book: <a class="reference external" href="https://link-springer-com.ezproxy.library.ubc.ca/book/10.1007/978-3-319-55456-3">Finite difference computing with PDES</a> by Hans Petter Langtangen and Svein Linge The entire book is available on <a class="reference external" href="http://hplgit.github.io/fdm-book/doc/pub/book/html/decay-book.html">github</a> with the python code
<a class="reference external" href="https://github.com/hplgit/fdm-book/tree/master/src">here</a>. Much of the content of this lab is summarized in <a class="reference external" href="https://link-springer-com.ezproxy.library.ubc.ca/content/pdf/bbm%3A978-3-319-55456-3%2F1.pdf">Appendix B – truncation analysis</a></p>
<div class="section" id="Other-recommended-books">
<h3>Other recommended books<a class="headerlink" href="#Other-recommended-books" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><strong>Differential Equations:</strong></p>
<ul>
<li><p>Strang (1986), Chapter 6 (ODE’s).</p></li>
</ul>
</li>
<li><p><strong>Numerical Methods:</strong></p>
<ul>
<li><p>Strang (1986), Section 6.5 (a great overview of difference methods for initial value problems)</p></li>
<li><p>Burden and Faires (1981), Chapter 5 (a more in-depth analysis of the numerical methods and their accuracy and stability).</p></li>
<li><p>Newman (2013) Derivatives, round-off and truncation errors, Section 5.10 pp. 188-198. Forward Euler, mid-point and leapfrog methods, Chapter 8 pp. 327-335.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this headline">¶</a></h2>
<p>Remember from Lab #1  that you were introduced to three approximations to the first derivative of a function, <span class="math notranslate nohighlight">\(T^\prime(t)\)</span>. If the independent variable, <span class="math notranslate nohighlight">\(t\)</span>, is discretized at a sequence of N points, <span class="math notranslate nohighlight">\(t_i=t_0+i \Delta t\)</span>, where <span class="math notranslate nohighlight">\(i = 0,1,\ldots, N\)</span> and <span class="math notranslate nohighlight">\(\Delta t= 1/N\)</span>, then we can write the three approximations as follows:</p>
<p><strong>Forward difference formula:</strong></p>
<div class="math notranslate nohighlight">
\[T^\prime(t_i) \approx \frac{T_{i+1}-T_i}{\Delta t}\]</div>
<p><strong>Backward difference formula:</strong></p>
<div class="math notranslate nohighlight">
\[T^\prime(t_i) \approx \frac{T_{i}-T_{i-1}}{\Delta t}\]</div>
<p><strong>Centered difference formula:</strong></p>
<div class="math notranslate nohighlight">
\[T^\prime(t_i) \approx \frac{T_{i+1}-T_{i-1}}{2 \Delta t}\]</div>
<p>In fact, there are many other possible methods to approximate the derivative (some of which we will see later in this Lab). With this large choice we have in the choice of approximation scheme, it is not at all clear at this point which, if any, of the schemes is the “best”. It is the purpose of this Lab to present you with some basic tools that will help you to decide on an appropriate discretization for a given problem. There is no generic “best” method, and the choice of discretization will
always depend on the problem that is being dealt with.</p>
<p>In an example from Lab #1, the forward difference formula was used to compute solutions to the saturation development equation, and you saw two important results:</p>
<ul class="simple">
<li><p>reducing the grid spacing, <span class="math notranslate nohighlight">\(\Delta t\)</span>, seemed to improve the accuracy of the approximate solution; and</p></li>
<li><p>if <span class="math notranslate nohighlight">\(\Delta t\)</span> was taken too large (that is, the grid was not fine enough), then the approximate solution exhibited non-physical oscillations, or a <em>numerical instability</em>.</p></li>
</ul>
<p>There are several questions that arise from this example:</p>
<ol class="arabic simple">
<li><p>Is it always true that reducing <span class="math notranslate nohighlight">\(\Delta t\)</span> will improve the discrete solution?</p></li>
<li><p>Is it possible to improve the accuracy by using another approximation scheme (such as one based on the backward or centered difference formulas)?</p></li>
<li><p>Are these numerical instabilities something that always appear when the grid spacing is too large?</p></li>
<li><p>By using another difference formula for the first derivative, is it possible to improve the stability of the approximate solution, or to eliminate the stability altogether?</p></li>
</ol>
<p>The first two questions, related to <em>accuracy</em>, will be dealt with in <a class="reference external" href="#Accuracy">Section 4</a>, and the last two will have to wait until <a class="reference external" href="#Stability">Section 5</a> when <em>stability</em> is discussed.</p>
</div>
<div class="section" id="Accuracy-of-Difference-Approximations">
<h2>Accuracy of Difference Approximations<a class="headerlink" href="#Accuracy-of-Difference-Approximations" title="Permalink to this headline">¶</a></h2>
<p>Before moving on to the details of how to measure the error in a scheme, let’s take a closer look at another example which we’ve seen already …</p>
<p>Let’s go back to the heat conduction equation from Lab #1, where the temperature, <span class="math notranslate nohighlight">\(T(t)\)</span>, of a rock immersed in water or air, evolves in time according to the first order ODE:</p>
<div class="math notranslate nohighlight">
\[\frac{dT}{dt} = \lambda(T,t) \, (T-T_a)\]</div>
<p>with initial condition <span class="math notranslate nohighlight">\(T(0)\)</span>. We saw in the section on the forward Euler method  that one way to discretize this equation was using the forward difference formula  for the derivative, leading to</p>
<p><span class="math notranslate nohighlight">\(T_{i+1} = T_i + \Delta t \, \lambda(T_i,t_i) \, (T_i-T_a).\)</span> (<strong>eq: euler</strong>)</p>
<p>Similarly, we could apply either of the other two difference formulae, or , to obtain other difference schemes, namely what we called the <em>backward Euler method</em></p>
<p><span class="math notranslate nohighlight">\(T_{i+1} = T_i + \Delta t \, \lambda(T_{i+1},t_{i+1}) \, (T_{i+1}-T_a),\)</span> (<strong>eq: beuler</strong>)</p>
<p>and the <em>mid-point</em> or <em>leap frog method</em></p>
<p><span class="math notranslate nohighlight">\(T_{i+1} = T_{i-1} + 2 \Delta t \, \lambda(T_{i},t_{i}) \, (T_{i}-T_a).\)</span> (<strong>eq: midpoint</strong>)</p>
<p>The forward Euler and mid-point schemes are called <em>explicit methods</em>, since they allow the temperature at any new time to be computed in terms of the solution values at previous time steps. The backward Euler scheme, on the other hand, is called an <em>implicit scheme</em>, since it gives an equation defining <span class="math notranslate nohighlight">\(T_{i+1}\)</span> implicitly (If <span class="math notranslate nohighlight">\(\lambda\)</span> depends non-linearly on <span class="math notranslate nohighlight">\(T\)</span>, then this equation may require an additional step, involving the iterative solution of a non-linear equation. We
will pass over this case for now, and refer you to a reference such as Burden and Faires for the details on non-linear solvers such as <em>Newton’s method</em>).</p>
<p><strong>Important point</strong>: Note that <strong>eq: midpoint</strong> requires the value of the temperature at two points: <span class="math notranslate nohighlight">\(T_{i-1}\)</span> and <span class="math notranslate nohighlight">\(T_{i}\)</span> to calculate the temperature <span class="math notranslate nohighlight">\(T_{i+1}\)</span>. This requires an approximate guess for <span class="math notranslate nohighlight">\(T_i\)</span>, which we will discuss in more detail below.</p>
<p>For now, let’s assume that <span class="math notranslate nohighlight">\(\lambda\)</span> is a constant, independent of <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(t\)</span>. Plots of the numerical results from each of these schemes, along with the exact solution, are given in Figure 1 (with the “unphysical” parameter value <span class="math notranslate nohighlight">\(\lambda=0.8\)</span> chosen to enhance the show the growth of numerical errors, even though in a real material this would violate conservation of energy).</p>
<p>The functions used in make the following figure are imported from <a class="reference external" href="https://github.com/phaustin/numeric/blob/master/numlabs/lab2/lab2_functions.py">lab2_functions.py</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">context</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">numlabs.lab2.lab2_functions</span> <span class="kn">import</span> <span class="n">euler</span><span class="p">,</span><span class="n">beuler</span><span class="p">,</span><span class="n">leapfrog</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="c1">#</span>
<span class="c1"># save our three functions to a dictionary, keyed by their names</span>
<span class="c1">#</span>
<span class="n">theFuncs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;euler&#39;</span><span class="p">:</span><span class="n">euler</span><span class="p">,</span><span class="s1">&#39;beuler&#39;</span><span class="p">:</span><span class="n">beuler</span><span class="p">,</span><span class="s1">&#39;leapfrog&#39;</span><span class="p">:</span><span class="n">leapfrog</span><span class="p">}</span>
<span class="c1">#</span>
<span class="c1"># store the results in another dictionary</span>
<span class="c1">#</span>
<span class="n">output</span><span class="o">=</span><span class="p">{}</span>
<span class="c1">#</span>
<span class="c1">#end time = 10 minutes</span>
<span class="c1">#</span>
<span class="n">tend</span><span class="o">=</span><span class="mf">10.</span>
<span class="c1">#</span>
<span class="c1"># start at 30 degC, air temp of 20 deg C</span>
<span class="c1">#</span>
<span class="n">Ta</span><span class="o">=</span><span class="mf">20.</span>
<span class="n">To</span><span class="o">=</span><span class="mf">30.</span>
<span class="c1">#</span>
<span class="c1"># note that lambda is a reserved keyword in python so call this</span>
<span class="c1"># thelambda</span>
<span class="c1">#</span>
<span class="n">theLambda</span><span class="o">=</span><span class="mf">0.8</span>  <span class="c1">#units have to be per minute if time in minutes</span>
<span class="c1">#</span>
<span class="c1"># dt = 10/npts = 10/30 = 1/3</span>
<span class="c1">#</span>
<span class="n">npts</span><span class="o">=</span><span class="mi">30</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">the_fun</span> <span class="ow">in</span> <span class="n">theFuncs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">output</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">=</span><span class="n">the_fun</span><span class="p">(</span><span class="n">npts</span><span class="p">,</span><span class="n">tend</span><span class="p">,</span><span class="n">To</span><span class="p">,</span><span class="n">Ta</span><span class="p">,</span><span class="n">theLambda</span><span class="p">)</span>
<span class="c1">#</span>
<span class="c1"># calculate the exact solution for comparison</span>
<span class="c1">#</span>
<span class="n">exactTime</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">tend</span><span class="p">,</span><span class="n">npts</span><span class="p">)</span>
<span class="n">exactTemp</span><span class="o">=</span><span class="n">Ta</span> <span class="o">+</span> <span class="p">(</span><span class="n">To</span><span class="o">-</span><span class="n">Ta</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">theLambda</span><span class="o">*</span><span class="n">exactTime</span><span class="p">)</span>
<span class="c1">#</span>
<span class="c1"># now plot all four curves</span>
<span class="c1">#</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">exactTime</span><span class="p">,</span><span class="n">exactTemp</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;exact&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">fun_name</span> <span class="ow">in</span> <span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">the_time</span><span class="p">,</span><span class="n">the_temp</span><span class="o">=</span><span class="n">output</span><span class="p">[</span><span class="n">fun_name</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">the_time</span><span class="p">,</span><span class="n">the_temp</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="n">fun_name</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">30.</span><span class="p">,</span><span class="mf">90.</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;time (minutes)&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;bar temp (deg C)&#39;</span><span class="p">)</span>
<span class="n">out</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
******************************
context imported. Front of path:
/Users/phil/repos/numeric
back of path: /Users/phil/.ipython
******************************

through /Users/phil/repos/numeric/notebooks/lab2/context.py
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/doc_notebooks_lab2_01-lab2_10_1.png" src="../../_images/doc_notebooks_lab2_01-lab2_10_1.png" />
</div>
</div>
<p><strong>Figure 1</strong> A plot of the exact and computed solutions for the temperature of a rock, with parameters: <span class="math notranslate nohighlight">\(T_a=20\)</span>, <span class="math notranslate nohighlight">\(T(0)=30\)</span>, <span class="math notranslate nohighlight">\(\lambda= +0.8\)</span>, <span class="math notranslate nohighlight">\(\Delta t=\frac{1}{3}\)</span></p>
<p>Notice from these results that the centered/leapfrog scheme is the most accurate, and backward Euler the least accurate.</p>
<p>The next section explains why some schemes are more accurate than others, and introduces a means to quantify the accuracy of a numerical approximation.</p>
<div class="section" id="Round-off-Error-and-Discretization-Error">
<h3>Round-off Error and Discretization Error<a class="headerlink" href="#Round-off-Error-and-Discretization-Error" title="Permalink to this headline">¶</a></h3>
<p>From <a class="reference external" href="#ex_accuracy">Example Accuracy</a> and the example in the Forward Euler section of the previous lab,  it is obvious that a numerical approximation is exactly that - <strong>an approximation</strong>. The process of discretizing a differential equation inevitably leads to errors. In this section, we will tackle two fundamental questions related to the accuracy of a numerical approximation:</p>
<ul class="simple">
<li><p>Where does the error come from (and how can we measure it)?</p></li>
<li><p>How can the error be controlled?</p></li>
</ul>
<div class="section" id="Where-does-the-error-come-from?">
<h4>Where does the error come from?<a class="headerlink" href="#Where-does-the-error-come-from?" title="Permalink to this headline">¶</a></h4>
<p>#####Round-off error:</p>
<p>When attempting to solve differential equations on a computer, there are two main sources of error. The first, <em>round-off error</em>, derives from the fact that a computer can only represent real numbers by <em>floating point</em> approximations, which have only a finite number of digits of accuracy.</p>
<ul class="simple">
<li><p>Mathematical note <a class="reference external" href="#floating-point">floating point notation</a></p></li>
</ul>
<p>For example, we all know that the number <span class="math notranslate nohighlight">\(\pi\)</span> is a non-repeating decimal, which to the first twenty significant digits is <span class="math notranslate nohighlight">\(3.1415926535897932385\dots\)</span> Imagine a computer which stores only eight significant digits, so that the value of <span class="math notranslate nohighlight">\(\pi\)</span> is rounded to <span class="math notranslate nohighlight">\(3.1415927\)</span>.</p>
<p>In many situations, these five digits of accuracy may be sufficient. However, in some cases, the results can be catastrophic, as shown in the following example:</p>
<div class="math notranslate nohighlight">
\[\frac{\pi}{(\pi + 0.00000001)-\pi}.\]</div>
<p>Since the computer can only “see” 8 significant digits, the addition <span class="math notranslate nohighlight">\(\pi+0.00000001\)</span> is simply equal to <span class="math notranslate nohighlight">\(\pi\)</span> as far as the computer is concerned. Hence, the computed result is <span class="math notranslate nohighlight">\(\frac{1}{0}\)</span> - an undefined expression! The exact answer <span class="math notranslate nohighlight">\(100000000\pi\)</span>, however, is a very well-defined non-zero value.</p>
<div class="section" id="Truncation-error:">
<h5>Truncation error:<a class="headerlink" href="#Truncation-error:" title="Permalink to this headline">¶</a></h5>
<p>The second source of error stems from the discretization of the problem, and hence is called <em>discretization error</em> or <em>truncation error</em>. In comparison, round-off error is always present, and is independent of the discretization being used. The simplest and most common way to analyse the truncation error in a scheme is using <em>Taylor series expansions</em>.</p>
<p>Let us begin with the forward difference formula for the first derivative, , which involves the discrete solution at times <span class="math notranslate nohighlight">\(t_{i+1}\)</span> and <span class="math notranslate nohighlight">\(t_{i}\)</span>. Since only continuous functions can be written as Taylor series, we expand the exact solution (instead of the discrete values <span class="math notranslate nohighlight">\(T_i\)</span>) at the discrete point <span class="math notranslate nohighlight">\(t_{i+1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[T(t_{i+1}) = T(t_i+\Delta t) = T(t_i) + (\Delta t) T^\prime(t_i) +
  \frac{1}{2}(\Delta t)^2 T^{\prime\prime}(t_i) +\cdots\]</div>
<p>Rewriting to clean this up slightly gives <strong>eq: feuler</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
T(t_{i+1}) &amp;= T(t_i) + \Delta t T^{\prime}(t_i,T(t_i)) +
  \underbrace{\frac{1}{2}(\Delta t)^2T^{\prime\prime}(t_i) + \cdots}
_{\mbox{ truncation error}} \\ \;
  &amp;= T(t_i) + \Delta t T^{\prime}(t_i) + {\cal O}(\Delta t^2).
 \end{aligned}\end{split}\]</div>
<p>This second expression writes the truncation error term in terms of <em>order notation</em>. If we write <span class="math notranslate nohighlight">\(y = {\cal O}(\Delta t)\)</span>, then we mean simply that <span class="math notranslate nohighlight">\(y &lt; c \cdot \Delta t\)</span> for some constant <span class="math notranslate nohighlight">\(c\)</span>, and we say that “ <span class="math notranslate nohighlight">\(y\)</span> is first order in <span class="math notranslate nohighlight">\(\Delta t\)</span> ” (since it depends on <span class="math notranslate nohighlight">\(\Delta t\)</span> to the first power) or “ <span class="math notranslate nohighlight">\(y\)</span> is big-oh of <span class="math notranslate nohighlight">\(\Delta t\)</span>.” As <span class="math notranslate nohighlight">\(\Delta t\)</span> is assumed small, the next term in the series, <span class="math notranslate nohighlight">\(\Delta t^2\)</span> is small compared to the
<span class="math notranslate nohighlight">\(\Delta t\)</span> term. In words, we say that forward euler is <em>first order accurate</em> with errors of second order.</p>
<p>It is clear from that as <span class="math notranslate nohighlight">\(\Delta t\)</span> is reduced in size (as the computational grid is refined), the error is also reduced. If you remember that we derived the approximation from the limit definition of derivative, then this should make sense. This dependence of the error on powers of the grid spacing <span class="math notranslate nohighlight">\(\Delta t\)</span> is an underlying characteristic of difference approximations, and we will see approximations with higher orders in the coming sections …</p>
<p>There is one more important distinction to be made here. The “truncation error” we have been discussing so far is actually what is called <em>local truncation error</em>. It is “local” in the sense that we have expanded the Taylor series <em>locally</em> about the exact solution at the point <span class="math notranslate nohighlight">\(t_i\)</span>.</p>
<p>There is also a <em>global truncation error</em> (or, simply, <em>global error</em>), which is the error made during the course of the entire computation, from time <span class="math notranslate nohighlight">\(t_0\)</span> to time <span class="math notranslate nohighlight">\(t_n\)</span>. The difference between local and global truncation error is illustrated in Figure 2. If the local error stays approximately constant, then the global error will be approximately the local error times the number of timesteps, or about one order of <span class="math notranslate nohighlight">\(\Delta t\)</span> worse than the local error.</p>
<p><strong>Figure Error:</strong> Local and global truncation error.</p>
<p>It is easy to get a handle on the order of the local truncation error using Taylor series, regardless of whether the exact solution is known, but no similar analysis is available for the global error. We can write</p>
<div class="math notranslate nohighlight">
\[\text{global error} = |T(t_n)-T_n|\]</div>
<p>but this expression can only be evaluated if the exact solution is known ahead of time (which is not the case in most problems we want to compute, since otherwise we wouldn’t be computing it in the first place!). Therefore, when we refer to truncation error, we will always be referring to the local truncation error.</p>
</div>
<div class="section" id="Second-order-accuracy">
<h5>Second order accuracy<a class="headerlink" href="#Second-order-accuracy" title="Permalink to this headline">¶</a></h5>
<p>Above we mentioned a problem with evaluating the mid-point method. If we start with three points <span class="math notranslate nohighlight">\((t_0,t_1,t_2)\)</span>, each separated by <span class="math notranslate nohighlight">\(\Delta t/2\)</span> so that <span class="math notranslate nohighlight">\(t_2 - t_0=\Delta t\)</span></p>
<p><span class="math">\begin{align}
y(t_2)&=y(t_1) + y^\prime (t_1,y(t_1))(t_2 - t_1) + \frac{y^{\prime \prime}(t_1,y(t_1))}{2} (t_2 - t_1)^2 + \frac{y^{\prime \prime \prime}(t_1,y(t_1))}{6} (t_2 - t_1)^3 + h.o.t. \ (eq.\ a)\\
y(t_0)&=y(t_1) + y^\prime (t_1,y(t_1))(t_0 - t_1) + \frac{y^{\prime \prime}(t_1)}{2} (t_0 - t_1)^2 + \frac{y^{\prime \prime \prime}(t_1)}{6} (t_0 - t_1)^3 + h.o.t. \ (eq.\ b)
\end{align}</span></p>
<p>where h.o.t. stands for “higher order terms”. Rewriting in terms of <span class="math notranslate nohighlight">\(\Delta t\)</span>:</p>
<p><span class="math">\begin{align}
y(t_2)&=y(t_1) + \frac{\Delta t}{2}y^\prime (t_1,y(t_1)) + \frac{\Delta t^2}{8} y^{\prime \prime}(t_1,y(t_1)) + \frac{\Delta t^3}{48} y^{\prime \prime \prime}(t_1,y(t_1)) + h.o.t. \ (eq.\ a)\\
y(t_0)&=y(t_1) - \frac{\Delta t}{2}y^\prime (t_1,y(t_1)) + \frac{\Delta t^2}{8} y^{\prime \prime}(t_1,y(t_1)) - \frac{\Delta t^3}{48} y^{\prime \prime \prime}(t_1,y(t_1)) + h.o.t. \ (eq.\ b)
\end{align}</span></p>
<p>and subtracting:</p>
<p><span class="math">\begin{align}
y(t_2)&=y(t_0) + \Delta t y^\prime (t_1,y(t_1))  + \frac{\Delta t^3}{24} y^{\prime \prime \prime}(t_1,y(t_1)) + h.o.t. \ (eq.\ c)
\end{align}</span></p>
<p>where <span class="math notranslate nohighlight">\(t_1=t_0 + \Delta t/2\)</span></p>
<p>Comparing with <a class="reference external" href="#eq:%20feuler">eq: feuler</a> we can see that we’ve canceled the <span class="math notranslate nohighlight">\(\Delta t^2\)</span> terms, so that if we drop the <span class="math notranslate nohighlight">\(\frac{\Delta t^3}{24} y^{\prime \prime \prime}(t_1,y(t_1))\)</span> and higher order terms we’re doing one order better that foward euler, as long as we can solve the problem of estimating y at the midpoint: <span class="math notranslate nohighlight">\(y(t_1) = y(t_0 + \Delta t/2)\)</span></p>
</div>
<div class="section" id="mid-point-and-leapfrog">
<h5>mid-point and leapfrog<a class="headerlink" href="#mid-point-and-leapfrog" title="Permalink to this headline">¶</a></h5>
<div class="line-block">
<div class="line">The mid-point and leapfrog methods take two slightly different approaches to estimating <span class="math notranslate nohighlight">\(y(t_0 + \Delta t/2)\)</span>.</div>
<div class="line">For the <a class="reference external" href="https://en.wikipedia.org/wiki/Midpoint_method">explicit mid-point method</a>, we estimate <span class="math notranslate nohighlight">\(y\)</span> at the midpoint by taking a half-step:</div>
</div>
<p><span class="math">\begin{align}
k_1 & = \Delta t y^\prime(t_0,y(t_0)) \\
k_2 & = \Delta t y^\prime(t_0 + \Delta t/2,y(t_0) + k_1/2) \\
y(t_0 + \Delta t) &= y(t_0) + k_2
\end{align}</span></p>
<p>Compare this to the <a class="reference external" href="https://en.wikipedia.org/wiki/Leapfrog_integration">leapfrog method</a>, which uses the results from one half-interval to calculate the results for the next half-interval:</p>
<p><span class="math">\begin{align}
y(t_0 + \Delta t/2)  & = y(t_0) + \frac{\Delta t}{2} y^\prime(t_0,y(t_0))\ (i) \\
y(t_0 + \Delta t) & = y(t_0) + \Delta t y^\prime(t_0 + \Delta t/2,y(t_0 + \Delta t/2)\ (ii)\\
y(t_0 + 3 \Delta t/2)  & = y(t_0 + \Delta t/2) + \Delta t y^\prime(t_0 + \Delta t,y(t_0 + \Delta t))\ (iii) \\
y(t_0 + 2 \Delta t)  & = y(t_0 + \Delta t) + \Delta t y^\prime(t_0 + 3\Delta t/2,y(t_0 + 3 \Delta t/2))\ (iv) \\
\end{align}</span></p>
<p>Comparing (iii) and (iv) shows how the method gets its name: the half-interval and whole interval values are calculated by leaping over each other. Once the first half and whole steps are done, the rest of the integration is completed by repeating (iii) and (iv) as until the endpoint is reached.</p>
<p>The leapfrog scheme has the advantage that it is <em>time reversible</em> or as the Wikipedia article says <em>sympletic</em>. This means that estimating <span class="math notranslate nohighlight">\(y(t_1)\)</span> and then using that value to go backwards by <span class="math notranslate nohighlight">\(-\Delta t\)</span> yields <span class="math notranslate nohighlight">\(y(t_0)\)</span> exactly, which the mid-point method does not. The mid-point method, however, is one member (the 2nd order member) of a family of <em>Runge Kutta</em> integrators, which will be covered in more detail in Lab 4.</p>
</div>
</div>
</div>
<div class="section" id="Problem-Error-–-hand-in-png-file-or-paper">
<h3>Problem Error – hand in png file or paper<a class="headerlink" href="#Problem-Error-–-hand-in-png-file-or-paper" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><ol class="loweralpha simple">
<li><p>Derive the error term for the backward difference formula using Taylor series, and hence show that it is also first order.</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="2">
<li><p>How does the constant in front of the leading order error term differ from that for the forward difference formula? Relate this back to the results plotted in <a class="reference external" href="#fig_error">Figure error</a>, where these two formulae were used to derive difference schemes for the heat conduction problem. (i.e., explain why backward euler is an overestimate and forward euler is an underestimate of the exact solution)</p></li>
</ol>
</li>
</ul>
<div class="section" id="How-can-we-control-the-error?">
<h4>How can we control the error?<a class="headerlink" href="#How-can-we-control-the-error?" title="Permalink to this headline">¶</a></h4>
<p>Now that we’ve determined the source of the error in numerical methods, we would like to find a way to control it; that is, we would like to be able to compute and be confident that our approximate solution is “close” to the exact solution. Round-off error is intrinsic to all numerical computations, and cannot be controlled (except to develop methods that do not magnify the error unduly … more on this later). Truncation error, on the other hand, <em>is</em> under our control.</p>
<p>In the simple ODE examples that we’re dealing with in this lab, the round-off error in a calculation is much smaller than the truncation error. Furthermore, the schemes being used are <em>stable with respect to round-off error</em> in the sense that round-off errors are not magnified in the course of a computation. So, we will restrict our discussion of error control in what follows to the truncation error.</p>
<p>However, there are many numerical algorithms in which the round-off error can dominate the the result of a computation (Gaussian elimination is one example, which you will see in Lab #3 ), and so we must always keep it in mind when doing numerical computations.</p>
<p>There are two fundamental ways in which the truncation error in an approximation  can be reduced:</p>
<ol class="arabic simple">
<li><p>Decrease the grid spacing, . Provided that the second derivative of the solution is bounded, it is clear from the error term in  that as   is reduced, the error will also get smaller. This principle was demonstrated in an example from Lab #1. The disadvantage to decreasing  is that the cost of the computation increases since more steps must be taken. Also, there is a limit to how small  can be, beyond which round-off errors will start polluting the computation.</p></li>
<li><p>Increase the order of the approximation. We saw above that the forward difference approximation of the first derivative is first order accurate in the grid spacing. It is also possible to derive higher order difference formulas which have a leading error term of the form <span class="math notranslate nohighlight">\((\Delta t)^p\)</span>, with <span class="math notranslate nohighlight">\(p&gt;1\)</span>. As noted above in <a class="reference external" href="#sec_secondOrder">Section Second Order</a> in the midpoint formula is a second order scheme, and some further examples will be given in <a class="reference external" href="#sec_HigherOrderTaylor">Section Higher order
Taylor</a>. The main disadvantage to using very high order schemes is that the error term depends on higher derivatives of the solution, which can sometimes be very large – in this case, the stability of the scheme can be adversely affected (for more on this, see <a class="reference external" href="#sec_stability">Section Stability</a>.</p></li>
</ol>
<div class="section" id="Problem-Accuracy:">
<h5>Problem Accuracy:<a class="headerlink" href="#Problem-Accuracy:" title="Permalink to this headline">¶</a></h5>
<p>In order to investigate these two approaches to improving the accuracy of an approximation, you can use the code in <a class="reference external" href="https://github.com/phaustin/numeric/blob/master/numlabs/lab2/terror2.py">terror.py</a> to play with the solutions to the heat conduction equation. For a given function <span class="math notranslate nohighlight">\(\lambda(T)\)</span>, and specified parameter values, you should experiment with various time steps and schemes, and compare the computed results (Note: only the answers to the assigned questions need to be handed
in). Look at the different schemes (euler, leapfrog, midpoint, 4th order runge kutta) run them for various total times (tend) and step sizes (dt=tend/npts).</p>
<p>The three schemes that will be used here are forward Euler (first order), leap frog (second order) and the fourth order Runge-Kutta scheme (which will be introduced in Lab 4).</p>
<p>To hand in: Try three different step sizes for all three schemes for a total of 9 runs. It’s helpful to be able to change the axis limits to look at various parts of the plot.</p>
<p>Use your 9 results to answer parts a and b below. (The leap-frog scheme should give you quasi-pathological results, see the explanation at the end of section 5)</p>
<ul class="simple">
<li><ol class="loweralpha simple">
<li><p>Does increasing the order of the scheme, or decreasing the time step always improve the solution?</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="2">
<li><p>How would you compute the local truncation error from the error plot? And the global error? Do this on a plot for one set of parameters.</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="3">
<li><p>Similarly, how might you estimate the <em>order</em> of the local truncation error? The order of the global error? ( <strong>Hint:</strong> An order <span class="math notranslate nohighlight">\(p\)</span> scheme has truncation error that looks like <span class="math notranslate nohighlight">\(c\cdot(\Delta t)^p\)</span>. Read the error off the plots for several values of the grid spacing and use this to find <span class="math notranslate nohighlight">\(p\)</span>.) Are the local and global error significantly different? Why or why not?</p></li>
</ol>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="Other-Approximations-to-the-First-Derivative">
<h3>Other Approximations to the First Derivative<a class="headerlink" href="#Other-Approximations-to-the-First-Derivative" title="Permalink to this headline">¶</a></h3>
<p>The Taylor series method of deriving difference formulae for the first derivative is the simplest, and can be used to obtain approximations with even higher order than two. There are also many other ways to discretize the derivatives appearing in ODE’s, as shown in the following sections…</p>
<div class="section" id="Higher-Order-Taylor-Methods">
<h4>Higher Order Taylor Methods<a class="headerlink" href="#Higher-Order-Taylor-Methods" title="Permalink to this headline">¶</a></h4>
<p>As mentioned earlier, there are many other possible approximations to the first derivative using the Taylor series approach. The basic approach in these methods is as follows:</p>
<ol class="arabic simple">
<li><p>expand the solution in a Taylor series at one or more points surrounding the point where the derivative is to be approximated (for example, for the centered scheme, you used two points, <span class="math notranslate nohighlight">\(T(t_i+\Delta t)\)</span> and <span class="math notranslate nohighlight">\(T(t_i-\Delta t)\)</span>. You also have to make sure that you expand the series to a high enough order …</p></li>
<li><p>take combinations of the equations until the <span class="math notranslate nohighlight">\(T_i\)</span> (and possibly some other derivative) terms are eliminated, and all you’re left with is the first derivative term.</p></li>
</ol>
<p>One example is the fourth-order centered difference formula for the first derivative:</p>
<div class="math notranslate nohighlight">
\[\frac{-T(t_{i+2})+8T(t_{i+1})-8T(t_{i-1})+T(t_{i-2})}{12\Delta t} =
  T^\prime(t_i) + {\cal O}((\Delta t)^4)\]</div>
<p><strong>Quiz:</strong> Try the quiz at <a class="reference external" href="https://phaustin.github.io/numeric">this link</a> related to this higher order scheme.</p>
</div>
<div class="section" id="Predictor-Corrector-Methods">
<h4>Predictor-Corrector Methods<a class="headerlink" href="#Predictor-Corrector-Methods" title="Permalink to this headline">¶</a></h4>
<p>Another class of discretizations are called <em>predictor-corrector methods</em>. Implicit methods can be difficult or expensive to use because of the solution step, and so they are seldom used to integrate ODE’s. Rather, they are often used as the basis for predictor-corrector algorithms, in which a “prediction” for <span class="math notranslate nohighlight">\(T_{i+1}\)</span> based only on an explicit method is then “corrected” to give a better value by using this precision in an implicit method.</p>
<p>To see the basic idea behind these methods, let’s go back (once again) to the backward Euler method for the heat conduction problem which reads:</p>
<div class="math notranslate nohighlight">
\[T_{i+1} = T_{i} + \Delta t \, \lambda( T_{i+1}, t_{i+1} ) \, ( T_{i+1}
- T_a ).\]</div>
<p>Note that after applying the backward difference formula , all terms in the right hand side are evaluated at time <span class="math notranslate nohighlight">\(t_{i+1}\)</span>.</p>
<p>Now, <span class="math notranslate nohighlight">\(T_{i+1}\)</span> is defined implicitly in terms of itself, and unless <span class="math notranslate nohighlight">\(\lambda\)</span> is a very simple function, it may be very difficult to solve this equation for the value of <span class="math notranslate nohighlight">\(T\)</span> at each time step. One alternative, mentioned already, is the use of a non-linear equation solver such as Newton’s method to solve this equation. However, this is an iterative scheme, and can lead to a lot of extra expense. A cheaper alternative is to realize that we could try estimating or <em>predicting</em> the
value of <span class="math notranslate nohighlight">\(T_{i+1}\)</span> using the simple explicit forward Euler formula and then use this in the right hand side, to obtain a <em>corrected</em> value of <span class="math notranslate nohighlight">\(T_{i+1}\)</span>. The resulting scheme,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll}
  \mathbf{Prediction}: &amp; \widetilde{T}_{i+1} = T_i + \Delta t \,
  \lambda(T_i,t_i) \, (T_i-T_a), \\ \; \\
  \mathbf{Correction}: &amp; T_{i+1} = T_i + \Delta t \,
  \lambda(\widetilde{T}_{i+1},t_{i+1}) \, (\widetilde{T}_{i+1}-T_a).
\end{array}\end{split}\]</div>
<p>This method is an explicit scheme, which can also be shown to be second order accurate in . It is the simplest in a whole class of schemes called <em>predictor-corrector schemes</em> (more information is available on these methods in a numerical analysis book such as  &#64;burden-faires).</p>
</div>
<div class="section" id="Other-Methods">
<h4>Other Methods<a class="headerlink" href="#Other-Methods" title="Permalink to this headline">¶</a></h4>
<p>The choice of methods is made even greater by two other classes of schemes:</p>
<p><strong>Runge-Kutta methods:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">We</span> <span class="n">have</span> <span class="n">already</span> <span class="n">seen</span> <span class="n">two</span> <span class="n">examples</span> <span class="n">of</span> <span class="n">the</span> <span class="n">Runge</span><span class="o">-</span><span class="n">Kutta</span> <span class="n">family</span> <span class="n">of</span> <span class="n">integrators</span><span class="p">:</span>  <span class="n">Forward</span> <span class="n">Euler</span> <span class="ow">is</span>
<span class="n">a</span> <span class="n">first</span> <span class="n">order</span> <span class="n">Runge</span><span class="o">-</span><span class="n">Kutta</span><span class="p">,</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">midpoint</span> <span class="n">method</span> <span class="ow">is</span> <span class="n">second</span> <span class="n">order</span> <span class="n">Runge</span><span class="o">-</span><span class="n">Kutta</span><span class="o">.</span>  <span class="n">Fourth</span>
<span class="ow">and</span> <span class="n">fifth</span> <span class="n">order</span> <span class="n">Runge</span><span class="o">-</span><span class="n">Kutta</span> <span class="n">algorithms</span>
<span class="n">which</span> <span class="n">will</span> <span class="n">be</span> <span class="n">described</span> <span class="ow">in</span> <span class="n">Labs</span> <span class="c1">#4 and #5</span>
</pre></div>
</div>
<p><strong>Multi-step methods:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">which</span> <span class="n">use</span> <span class="n">values</span> <span class="n">of</span> <span class="n">the</span> <span class="n">solution</span> <span class="n">at</span> <span class="n">more</span> <span class="n">than</span> <span class="n">one</span> <span class="n">previous</span> <span class="n">time</span> <span class="n">step</span>
<span class="ow">in</span> <span class="n">order</span> <span class="n">to</span> <span class="n">increase</span> <span class="n">the</span> <span class="n">accuracy</span><span class="o">.</span> <span class="n">Compare</span> <span class="n">these</span> <span class="n">to</span> <span class="n">one</span><span class="o">-</span><span class="n">step</span>
<span class="n">schemes</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">forward</span> <span class="n">Euler</span><span class="p">,</span> <span class="n">which</span> <span class="n">use</span> <span class="n">the</span> <span class="n">solution</span> <span class="n">only</span> <span class="n">at</span> <span class="n">one</span>
<span class="n">previous</span> <span class="n">step</span><span class="o">.</span>
</pre></div>
</div>
<p>More can be found on these (and other) methods in  Burden and Faires and Newman.</p>
</div>
</div>
<div class="section" id="Accuracy-Summary">
<h3>Accuracy Summary<a class="headerlink" href="#Accuracy-Summary" title="Permalink to this headline">¶</a></h3>
<p>In this section, you’ve been given a short overview of the accuracy of difference schemes for first order ordinary differential equations. We’ve seen that accuracy can be improved by either decreasing the grid spacing, or by choosing a higher order scheme from one of several classes of methods. When using a higher order scheme, it is important to realize that the cost of the computation usually rises due to an added number of function evaluations (especially for multi-step and Runge-Kutta
methods). When selecting a numerical scheme, it is important to keep in mind this trade-off between accuracy and cost.</p>
<p>However, there is another important aspect of discretization that we have pretty much ignored. The next section will take a look at schemes of various orders from a different light, namely that of <em>stability</em>.</p>
</div>
</div>
<div class="section" id="Stability-of-Difference-Approximations">
<h2>Stability of Difference Approximations<a class="headerlink" href="#Stability-of-Difference-Approximations" title="Permalink to this headline">¶</a></h2>
<p>The easiest way to introduce the concept of stability is for you to see it yourself.</p>
<p>This example is a slight modification of <a class="reference external" href="#Problem-Accuracy">Problem accuracy</a> from the previous section on accuracy. We will add one scheme (backward euler) and drop the 4th order Runge-Kutta, and change the focus from error to stability. The value of <span class="math notranslate nohighlight">\(\lambda\)</span> is assumed a constant, so that the backward Euler scheme results in an explicit method, and we’ll also compute a bit further in time, so that any instability manifests itself more clearly. Run the
<a class="reference external" href="https://github.com/phaustin/numeric/blob/master/numlabs/lab2/stability2.py">stability2.py</a> script in ipython or the notebook with <span class="math notranslate nohighlight">\(\lambda= -8\ s^{-1}\)</span>, with <span class="math notranslate nohighlight">\(\Delta t\)</span> values that just straddle the stability condition for the forward euler scheme (<span class="math notranslate nohighlight">\(\Delta t &lt; \frac{-2}{\lambda}\)</span>, derived below). Hand in plots with comments that show that 1) the stability condition does in fact predict the onset of the instablity in the euler scheme, and 2) the backward euler and leapfrog
are either stable or unstable for the same <span class="math notranslate nohighlight">\(\Delta t\)</span> values. (you should run out to longer than tend=10 seconds to see if there is a delayed instability.)</p>
<p>The heat conduction problem, as you saw in Lab #1, has solutions that are stable when <span class="math notranslate nohighlight">\(\lambda&lt;0\)</span>. It is clear from <a class="reference external" href="#Problem-Stability">Problem stability</a> above that some higher order schemes (namely, the leap-frog scheme) introduce a spurious oscillation not present in the continuous solution. This is called a <em>computational</em> or <em>numerical instability</em>, because it is an artifact of the discretization process only. This instability is not a characteristic of the heat conduction problem
alone, but is present in other problems where such schemes are used. Furthermore, as we will see below, even a scheme such as forward Euler can be unstable for certain problems and choices of the time step.</p>
<p>There is a way to determine the stability properties of a scheme, and that is to apply the scheme to the <em>test equation</em></p>
<div class="math notranslate nohighlight">
\[\frac{dz}{dt} = \lambda z\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is a complex constant.</p>
<p>The reason for using this equation may not seem very clear. But if you think in terms of <span class="math notranslate nohighlight">\(\lambda z\)</span> as being the linearization of some more complex right hand side, then the solution to is <span class="math notranslate nohighlight">\(z=e^{\lambda t}\)</span>, and so <span class="math notranslate nohighlight">\(z\)</span> represents, in some sense, a Fourier mode of the solution to the linearized ODE problem. We expect that the behaviour of the simpler, linearized problem should mimic that of the original problem.</p>
<p>Applying the forward Euler scheme to this test equation, results in the following difference formula</p>
<div class="math notranslate nohighlight">
\[z_{i+1} = z_i+(\lambda \Delta t)z_i\]</div>
<p>which is a formula that we can apply iteratively to <span class="math notranslate nohighlight">\(z_i\)</span> to obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
z_{i+1} &amp;=&amp; (1+\lambda \Delta t)z_{i} \\
        &amp;=&amp; (1+\lambda \Delta t)^2 z_{i-1} \\
        &amp;=&amp; \cdots \\
        &amp;=&amp; (1+\lambda \Delta t)^{i+1} z_{0}.\end{aligned}\end{split}\]</div>
<p>The value of <span class="math notranslate nohighlight">\(z_0\)</span> is fixed by the initial conditions, and so this difference equation for <span class="math notranslate nohighlight">\(z_{i+1}\)</span> will “blow up” as <span class="math notranslate nohighlight">\(i\)</span> gets bigger, if the factor in front of <span class="math notranslate nohighlight">\(z_0\)</span> is greater than 1 in magnitude – this is a sign of instability. Hence, this analysis has led us to the conclusion that if</p>
<div class="math notranslate nohighlight">
\[|1+\lambda\Delta t| &lt; 1,\]</div>
<p>then the forward Euler method is stable. For <em>real</em> values of <span class="math notranslate nohighlight">\(\lambda&lt;0\)</span>, this inequality can be shown to be equivalent to the <em>stability condition</em></p>
<div class="math notranslate nohighlight">
\[\Delta t &lt; \frac{-2}{\lambda},\]</div>
<p>which is a restriction on how large the time step can be so that the numerical solution is stable.</p>
<p>Perform a similar analysis for the backward Euler formula, and show that it is <em>always stable</em> when <span class="math notranslate nohighlight">\(\lambda\)</span> is real and negative.</p>
<p><em>Now, what about the leap frog scheme?</em></p>
<p>Applying the test equation to the leap frog scheme results in the difference equation</p>
<div class="math notranslate nohighlight">
\[z_{i+1} = z_{i-1} + 2 \lambda \Delta t z_i.\]</div>
<p>Difference formulas such as this one are typically solved by looking for a solution of the form <span class="math notranslate nohighlight">\(z_i = w^i\)</span> which, when substituted into this equation, yields</p>
<div class="math notranslate nohighlight">
\[w^2 - 2\lambda\Delta t w - 1 = 0,\]</div>
<p>a quadratic equation with solution</p>
<div class="math notranslate nohighlight">
\[w = \lambda \Delta t \left[ 1 \pm \sqrt{1+\frac{1}{(\lambda
        \Delta t)^2}} \right].\]</div>
<p>The solution to the original difference equation, <span class="math notranslate nohighlight">\(z_i=w^i\)</span> is stable only if all solutions to this quadratic satisfy <span class="math notranslate nohighlight">\(|w|&lt;1\)</span>, since otherwise, <span class="math notranslate nohighlight">\(z_i\)</span> will blow up as <span class="math notranslate nohighlight">\(i\)</span> gets large.</p>
<p>The mathematical details are not important here – what is important is that there are two (possibly complex) roots to the quadratic equation for <span class="math notranslate nohighlight">\(w\)</span>, and one is <em>always</em> greater than 1 in magnitude <em>unless</em> <span class="math notranslate nohighlight">\(\lambda\)</span> is pure imaginary ( has real part equal to zero), <em>and</em> <span class="math notranslate nohighlight">\(|\lambda \Delta t|&lt;1\)</span>. For the heat conduction equation in <a class="reference external" href="#prob_stability">Problem Stability</a> (which is already of the same form as the test equation ), <span class="math notranslate nohighlight">\(\lambda\)</span> is clearly not imaginary, which
explains the presence of the instability for the leap-frog scheme.</p>
<p>Nevertheless, the leap frog scheme is still useful for computations. In fact, it is often used in geophysical applications, as you will see later on when discretizing , and .</p>
<p>An example of where the leap frog scheme is superior to the other first order schemes is for undamped periodic motion (which arose in the weather balloon example from Lab #1 ). This corresponds to the system of ordinary differential equations (with the damping parameter, <span class="math notranslate nohighlight">\(\beta\)</span>, taken to be zero):</p>
<div class="math notranslate nohighlight">
\[\frac{dy}{dt} = u,\]</div>
<div class="math notranslate nohighlight">
\[\frac{du}{dt} = - \frac{\gamma}{m} y.\]</div>
<p>You’ve already discretized this problem using the forward difference formula, and the same can be done with the second order centered formula. We can then compare the forward Euler and leap-frog schemes applied to this problem. We code this in the module</p>
<p>Solution plots are given in <a class="reference external" href="#fig_oscillator">Figure oscilator</a>, for parameters <span class="math notranslate nohighlight">\(\gamma/m=1\)</span>, <span class="math notranslate nohighlight">\(\Delta t=0.25\)</span>, <span class="math notranslate nohighlight">\(y(0)=0.0\)</span> and <span class="math notranslate nohighlight">\(u(0)=1.0\)</span>, and demonstrate that the leap-frog scheme is stable, while forward Euler is unstable. This can easily be explained in terms of the stability criteria we derived for the two schemes when applied to the test equation. The undamped oscillator problem is a linear problem with pure imaginary eigenvalues, so as long as
<span class="math notranslate nohighlight">\(|\sqrt{\gamma/m}\Delta t|&lt;1\)</span>, the leap frog scheme is stable, which is obviously true for the parameter values we are given. Furthermore, the forward Euler stability condition <span class="math notranslate nohighlight">\(|1+\lambda\Delta  t|&lt;1\)</span> is violated for any choice of time step (when <span class="math notranslate nohighlight">\(\lambda\)</span> is pure imaginary) and so this scheme is always unstable for the undamped oscillator. The github link to the oscillator module is <a class="reference external" href="https://github.com/phaustin/numeric/blob/master/numlabs/lab2/oscillator.py">oscillator.py</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numlabs.lab2.oscillator</span> <span class="k">as</span> <span class="nn">os</span>
<span class="n">the_times</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">20.</span><span class="p">,</span><span class="mi">80</span><span class="p">)</span>
<span class="n">yvec_init</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">output_euler</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">euler</span><span class="p">(</span><span class="n">the_times</span><span class="p">,</span><span class="n">yvec_init</span><span class="p">)</span>
<span class="n">output_mid</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">midpoint</span><span class="p">(</span><span class="n">the_times</span><span class="p">,</span><span class="n">yvec_init</span><span class="p">)</span>
<span class="n">output_leap</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">leapfrog</span><span class="p">(</span><span class="n">the_times</span><span class="p">,</span><span class="n">yvec_init</span><span class="p">)</span>
<span class="n">answer</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">the_times</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">the_times</span><span class="p">,(</span><span class="n">output_euler</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">-</span><span class="n">answer</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;euler&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">the_times</span><span class="p">,(</span><span class="n">output_mid</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">-</span><span class="n">answer</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;midpoint&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">the_times</span><span class="p">,(</span><span class="n">output_leap</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">-</span><span class="n">answer</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;leapfrog&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;global error between sin(t) and approx. for three schemes&#39;</span><span class="p">,</span>
      <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;time&#39;</span><span class="p">,</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;exact - approx&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/doc_notebooks_lab2_01-lab2_32_0.png" src="../../_images/doc_notebooks_lab2_01-lab2_32_0.png" />
</div>
</div>
<p><strong>Figure numerical</strong>: Numerical solution to the undamped harmonic oscillator problem, using the forward Euler and leap-frog schemes. Parameter values: <span class="math notranslate nohighlight">\(\gamma / m=1.0\)</span>, <span class="math notranslate nohighlight">\(\Delta t=0.25\)</span>, <span class="math notranslate nohighlight">\(y(0)=0\)</span>, <span class="math notranslate nohighlight">\(u(0)=1.0\)</span>. The exact solution is a sinusoidal wave.</p>
<p>Had we taken a larger time step (such as <span class="math notranslate nohighlight">\(\Delta t=2.0\)</span>, for example), then even the leap-frog scheme is unstable. Furthermore, if we add damping (<span class="math notranslate nohighlight">\(\beta\neq 0\)</span>), then the eigenvalues are no longer pure imaginary, and the leap frog scheme is unstable no matter what time step we use.</p>
</div>
<div class="section" id="Stiff-Equations">
<h2>Stiff Equations<a class="headerlink" href="#Stiff-Equations" title="Permalink to this headline">¶</a></h2>
<p>One final note: this Lab has dealt only with ODE’s (and systems of ODE’s) that are <em>non-stiff</em>. <em>Stiff equations</em> are equations that have solutions with at least two widely varying times scales over which the solution changes. An example of stiff solution behaviour is a problem with solutions that have rapid, transitory oscillations, which die out over a short time scale, after which the solution slowly decays to an equilibrium. A small time step is required in the initial transitory region in
order to capture the rapid oscillations. However, a larger time step can be taken in the non-oscillatory region where the solution is smoother. Hence, using a very small time step will result in very slow and inefficient computations.</p>
<p>There are also many other numerical schemes designed specifically for stiff equations, most of which are implicit schemes. We will not describe any of them here – you can find more information in a numerical analysis text such as  &#64;burden-faires.</p>
</div>
<div class="section" id="Difference-Approximations-of-Higher-Derivatives">
<h2>Difference Approximations of Higher Derivatives<a class="headerlink" href="#Difference-Approximations-of-Higher-Derivatives" title="Permalink to this headline">¶</a></h2>
<p>Higher derivatives can be discretized in a similar way to what we did for first derivatives. Let’s consider for now only the second derivative, for which one possible approximation is the second order centered formula:</p>
<div class="math notranslate nohighlight">
\[\frac{y(t_{i+1})-2y(t_i)+y(t_{i-1})}{(\Delta t)^2} =
  y^{\prime\prime}(t_i) + {\cal O}((\Delta t)^2),\]</div>
<p>There are, of course, many other possible formulae that we might use, but this is the most commonly used.</p>
<ul class="simple">
<li><ol class="loweralpha simple">
<li><p>Use Taylor series to derive this formula.</p></li>
</ol>
</li>
<li><ol class="loweralpha simple" start="2">
<li><p>Derive a higher order approximation.</p></li>
</ol>
</li>
</ul>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h2>
<p>This lab has discussed the accuracy and stability of difference schemes for simple first order ODEs. The results of the problems should have made it clear to you that choosing an accurate and stable discretization for even a very simple problem is not straightforward. One must take into account not only the considerations of accuracy and stability, but also the cost or complexity of the scheme. Selecting a numerical method for a given problem can be considered as an art in itself.</p>
</div>
<div class="section" id="Mathematical-Notes">
<h2>Mathematical Notes<a class="headerlink" href="#Mathematical-Notes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Taylor-Polynomials-and-Taylor-Series">
<h3>Taylor Polynomials and Taylor Series<a class="headerlink" href="#Taylor-Polynomials-and-Taylor-Series" title="Permalink to this headline">¶</a></h3>
<p>Taylor Series are of fundamental importance in numerical analysis. They are the most basic tool for talking about the approximation of functions. Consider a function <span class="math notranslate nohighlight">\(f(x)\)</span> that is smooth – when we say “smooth”, what we mean is that its derivatives exist and are bounded (for the following discussion, we need <span class="math notranslate nohighlight">\(f\)</span> to have <span class="math notranslate nohighlight">\((n+1)\)</span> derivatives). We would like to approximate <span class="math notranslate nohighlight">\(f(x)\)</span> near the point <span class="math notranslate nohighlight">\(x=x_0\)</span>, and we can do it as follows:</p>
<div class="math notranslate nohighlight">
\[f(x) = \underbrace{P_n(x)}_{\mbox{Taylor polynomial}} +
  \underbrace{R_n(x)}_{\mbox{remainder term}},\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[P_n(x)=f(x_0)+ f^\prime(x_0)(x-x_0) +
  \frac{f^{\prime\prime}(x_0)}{2!}(x-x_0)^2 + \cdots +
  \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n\]</div>
<p>is the <em>:math:`n`th order Taylor polynomial</em> of <span class="math notranslate nohighlight">\(f\)</span> about <span class="math notranslate nohighlight">\(x_0\)</span>, and</p>
<div class="math notranslate nohighlight">
\[R_n(x)=\frac{f^{(n+1)}(\xi(x))}{(n+1)!}(x-x_0)^{n+1}\]</div>
<p>is the <em>remainder term</em> or <em>truncation error</em>. The point <span class="math notranslate nohighlight">\(\xi(x)\)</span> in the error term lies somewhere between the points <span class="math notranslate nohighlight">\(x_0\)</span> and <span class="math notranslate nohighlight">\(x\)</span>. If we look at the infinite sum ( let <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>), then the resulting infinite sum is called the <em>Taylor series of :math:`f(x)` about :math:`x=x_0`</em>. This result is also know as <em>Taylor’s Theorem</em>.</p>
<p>Remember that we assumed that <span class="math notranslate nohighlight">\(f(x)\)</span> is smooth (in particular, that its derivatives up to order <span class="math notranslate nohighlight">\((n+1)\)</span> exist and are finite). That means that all of the derivatives appearing in <span class="math notranslate nohighlight">\(P_n\)</span> and <span class="math notranslate nohighlight">\(R_n\)</span> are bounded. Therefore, there are two ways in which we can think of the Taylor polynomial <span class="math notranslate nohighlight">\(P_n(x)\)</span> as an approximation of <span class="math notranslate nohighlight">\(f(x)\)</span>:</p>
<ol class="arabic simple">
<li><p>First of all, let us fix <span class="math notranslate nohighlight">\(n\)</span>. Then, we can improve the approximation by letting <span class="math notranslate nohighlight">\(x\)</span> approach <span class="math notranslate nohighlight">\(x_0\)</span>, since as <span class="math notranslate nohighlight">\((x-x_0)\)</span> gets small, the error term <span class="math notranslate nohighlight">\(R_n(x)\)</span> goes to zero (<span class="math notranslate nohighlight">\(n\)</span> is considered fixed and all terms depending on <span class="math notranslate nohighlight">\(n\)</span> are thus constant). Therefore, the approximation improves when <span class="math notranslate nohighlight">\(x\)</span> gets closer and closer to <span class="math notranslate nohighlight">\(x_0\)</span>.</p></li>
<li><p>Alternatively, we can think of fixing <span class="math notranslate nohighlight">\(x\)</span>. Then, we can improve the approximation by taking more and more terms in the series. When <span class="math notranslate nohighlight">\(n\)</span> is increased, the factorial in the denominator of the error term will eventually dominate the <span class="math notranslate nohighlight">\((x-x_0)^{n+1}\)</span> term (regardless of how big <span class="math notranslate nohighlight">\((x-x_0)\)</span> is), and thus drive the error to zero.</p></li>
</ol>
<p>In summary, we have two ways of improving the Taylor polynomial approximation to a function: by evaluating it at points closer to the point <span class="math notranslate nohighlight">\(x_0\)</span>; and by taking more terms in the series.</p>
<p>This latter property of the Taylor expansion can be seen by a simple example. Consider the Taylor polynomial for the function <span class="math notranslate nohighlight">\(f(x)=\sin(x)\)</span> about the point <span class="math notranslate nohighlight">\(x_0=0\)</span>. All of the even terms are zero since they involve <span class="math notranslate nohighlight">\(sin(0)\)</span>, so that if we take <span class="math notranslate nohighlight">\(n\)</span> odd ( <span class="math notranslate nohighlight">\(n=2k+1\)</span>), then the <span class="math notranslate nohighlight">\(n\)</span>th order Taylor polynomial for <span class="math notranslate nohighlight">\(sin(x)\)</span> is</p>
<div class="math notranslate nohighlight">
\[P_{2k+1}(x)=x - \frac{x^3}{3!}+\frac{x^5}{5!} -\frac{x^7}{7!}+\cdots
    +\frac{x^{2k+1}}{(2k+1)!}.\ eq: taylor\]</div>
<p>The plot in Figure: Taylor illustrates quite clearly how the approximation improves both as <span class="math notranslate nohighlight">\(x\)</span> approaches 0, and as <span class="math notranslate nohighlight">\(n\)</span> is increased.</p>
<p>(We’ll go over <a class="reference external" href="https://github.com/phaustin/numeric/blob/master/numlabs/lab2/taylor_sin.py">sin_taylor.py</a>, the python module that generated Figure: Taylor in class)</p>
<p>Figure: Taylor – Plot of <span class="math notranslate nohighlight">\(\sin(x)\)</span> compared to its Taylor polynomial approximations about <span class="math notranslate nohighlight">\(x_0=0\)</span>, for various values of <span class="math notranslate nohighlight">\(n=2k +1\)</span> in eq: taylor.</p>
<p>Consider a specific Taylor polynomial, say <span class="math notranslate nohighlight">\(P_3(x)\)</span> ( fix <span class="math notranslate nohighlight">\(n=3\)</span>). Notice that for <span class="math notranslate nohighlight">\(x\)</span> far away from the origin, the polynomial is nowhere near the function <span class="math notranslate nohighlight">\(\sin(x)\)</span>. However, it approximates the function quite well near the origin. On the other hand, we could take a specific point, <span class="math notranslate nohighlight">\(x=5\)</span>, and notice that the Taylor series of orders 1 through 7 do not approximate the function very well at all. Nevertheless the approximation improves as <span class="math notranslate nohighlight">\(n\)</span> increases, as is
shown by the 15th order Taylor polynomial.</p>
</div>
<div class="section" id="Floating-Point-Representation-of-Numbers">
<h3>Floating Point Representation of Numbers<a class="headerlink" href="#Floating-Point-Representation-of-Numbers" title="Permalink to this headline">¶</a></h3>
<p>Unlike a mathematician, who can deal with real numbers having infinite precision, a computer can represent numbers with only a finite number of digits. The best way to understand how a computer stores a number is to look at its <em>floating-point form</em>, in which a number is written as</p>
<div class="math notranslate nohighlight">
\[\pm 0.d_1 d_2 d_3 \ldots d_k \times 10^n,\]</div>
<p>where each digit, <span class="math notranslate nohighlight">\(d_i\)</span> is between 0 and 9 (except <span class="math notranslate nohighlight">\(d_1\)</span>, which must be non-zero). Floating point form is commonly used in the physical sciences to represent numerical values; for example, the Earth’s radius is approximately 6,400,000 metres, which is more conveniently written in floating point form as <span class="math notranslate nohighlight">\(0.64\times 10^7\)</span> (compare this to the general form above).</p>
<p>Computers actually store numbers in <em>binary form</em> (i.e. in base-2 floating point form, as compared to the decimal or base-10 form shown above). However, it is more convenient to use the decimal form in order to illustrate the basic idea of computer arithmetic. For a good discussion of the binary representation of numbers, see Burden &amp; Faires [sec. 1.2] or Newman section 4.2.</p>
<p>For the remainder of this discussion, assume that we’re dealing with a computer that can store numbers with up to 8 <em>significant digits</em> (i.e. <span class="math notranslate nohighlight">\(k=8\)</span>) and exponents in the range <span class="math notranslate nohighlight">\(-38 \leq n \leq 38\)</span>. Based on these values, we can make a few observations regarding the numbers that can be represented:</p>
<ul>
<li><p>The largest number that can be represented is about <span class="math notranslate nohighlight">\(1.0\times  10^{+38}\)</span>, while the smallest is <span class="math notranslate nohighlight">\(1.0\times 10^{-38}\)</span>.</p></li>
<li><p>These numbers have a lot of <em>holes</em>, where real numbers are missed. For example, consider the two consecutive floating point numbers</p>
<div class="math notranslate nohighlight">
\[0.13391482 \times 10^5 \;\;\; {\rm and} \;\;\; 0.13391483 \times 10^5,\]</div>
<p>or 13391.482 and 13391.483. Our floating-point number system cannot represent any numbers between these two values, and hence any number in between 13391.482 and 13391.483 must be approximated by one of the two values. Another way of thinking of this is to observe that <span class="math notranslate nohighlight">\(0.13391482 \times 10^5\)</span> does not represent just a single real number, but a whole <em>range</em> of numbers.</p>
</li>
<li><p>Notice that the same amount of floating-point numbers can be represented between <span class="math notranslate nohighlight">\(10^{-6}\)</span> and <span class="math notranslate nohighlight">\(10^{-5}\)</span> as are between <span class="math notranslate nohighlight">\(10^{20}\)</span> and <span class="math notranslate nohighlight">\(10^{21}\)</span>. Consequently, the density of floating points numbers increases as their magnitude becomes smaller. That is, there are more floating-point numbers close to zero than there are far away. This is illustrated in the figure below.</p>
<p>The floating-point numbers (each represented by a <span class="math notranslate nohighlight">\(\times\)</span>) are more dense near the origin.</p>
</li>
</ul>
<p>The values <span class="math notranslate nohighlight">\(k=8\)</span> and <span class="math notranslate nohighlight">\(-38\leq n \leq 38\)</span> correspond to what is known as <em>single precision arithmetic</em>, in which 4 bytes (or units of memory in a computer) are used to store each number. It is typical in many programming languages, including <span class="math notranslate nohighlight">\(C++\)</span>, to allow the use of higher precision, or <em>double precision</em>, using 8 bytes for each number, corresponding to values of <span class="math notranslate nohighlight">\(k=16\)</span> and <span class="math notranslate nohighlight">\(-308\leq n \leq 308\)</span>, thereby greatly increasing the range and density of numbers that can
be represented. When doing numerical computations, it is customary to use double-precision arithmetic, in order to minimize the effects of round-off error (in a <span class="math notranslate nohighlight">\(C++\)</span> program, you can define a variable <code class="docutils literal notranslate"><span class="pre">x</span></code> to be double precision using the declaration <code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">x;</span></code>).</p>
<p>Sometimes, double precision arithmetic may help in eliminating round-off error problems in a computation. On the minus side, double precision numbers require more storage than their single precision counterparts, and it is sometimes (but not always) more costly to compute in double precision. Ultimately, though, using double precision should not be expected to be a cure-all against the difficulties of round-off errors. The best approach is to use an algorithm that is not unstable with respect to
round-off error. For an example where increasing precision will not help, see the section on Gaussian elimination in Lab #3.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../lab3/01-lab3.html" class="btn btn-neutral float-right" title="Laboratory 3: Linear Algebra (Sept. 12, 2017)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../lab1/01-lab1.html" class="btn btn-neutral float-left" title="Laboratory 1: An Introduction to the Numerical Solution of Differential Equations: Discretization (2020/Jan/6)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Numeric project

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>